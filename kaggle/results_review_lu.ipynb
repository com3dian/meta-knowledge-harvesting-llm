{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TASKNAMES = [\"subtask1\", \"subtask2\"]\n",
    "TYPES = [\"test\", \"test_preprocess\"]\n",
    "\n",
    "dataset_loader = dict()\n",
    "\n",
    "for taskname in TASKNAMES:\n",
    "    for type in TYPES:\n",
    "        print(f\"Processing {taskname} for {type}\")\n",
    "        prefix = \"dataset\"\n",
    "        # Load the data\n",
    "        \n",
    "        if taskname == \"subtask2\" and type == \"test_preprocess\":\n",
    "            continue\n",
    "        data = pd.read_csv(f\"{prefix}/{taskname}_{type}.csv\")\n",
    "        dataset_loader[f\"{taskname}_{type}\"] = data\n",
    "\n",
    "dataset_loader[\"subtask1_test\"][\"claim\"] = dataset_loader[\"subtask1_test_preprocess\"][\"claim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from rapidfuzz import fuzz\n",
    "from utils import get_latest_result_by_prefix\n",
    "\n",
    "from kg_compare import get_src_tgt_dict, get_map_src_tgt_dict, compare_knowledge_graphs_similarity\n",
    "test_data = dataset_loader[\"subtask1_test\"]\n",
    "\n",
    "index = 6\n",
    "claim_nodes, claim_edges = get_latest_result_by_prefix(f\"claim_test_{index}_\")\n",
    "reference_nodes, reference_edges = get_latest_result_by_prefix(f\"reference_test_{index}_\")\n",
    "reference_content = test_data.at[index, \"reference\"]\n",
    "\n",
    "claim_node_list = list(claim_nodes.keys())\n",
    "reference_node_list = list(reference_nodes.keys())\n",
    "\n",
    "if claim_node_list == [] or reference_node_list == []:\n",
    "    print(\"Claim or reference is empty\")\n",
    "    \n",
    "    \n",
    "similarity_matrix = compare_knowledge_graphs_similarity(claim_node_list,\n",
    "                                                        reference_node_list,\n",
    "                                                        if_plot=False)\n",
    "src_tgt_dict = get_src_tgt_dict(similarity_matrix, claim_node_list, reference_node_list)\n",
    "\n",
    "claim_reference_edge_map = get_map_src_tgt_dict(src_tgt_dict, claim_edges)\n",
    "\n",
    "def find_best_span_for_all_evidences(reference_edges,\n",
    "                                     claim_reference_edge_map,\n",
    "                                     paragraph,\n",
    "                                     max_window_size=5):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "\n",
    "    best_span_list = []\n",
    "    for _, reference_edge_list in claim_reference_edge_map.items():\n",
    "        evidence_edges = set(reference_edge_list) & set(reference_edges.keys())\n",
    "\n",
    "        for evidence_edge in evidence_edges:\n",
    "            reference_edge = reference_edges[evidence_edge]\n",
    "\n",
    "            best_score = -1\n",
    "            best_span = None\n",
    "\n",
    "            for window_size in range(1, min(max_window_size, len(sentences)) + 1):\n",
    "                for i in range(len(sentences) - window_size + 1):\n",
    "                    window = ' '.join(sentences[i:i+window_size])\n",
    "                    score = fuzz.ratio(reference_edge[0][\"description\"], window)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_span = (i, i+window_size)\n",
    "            best_span_list.append(best_span)\n",
    "\n",
    "\n",
    "    intervals = sorted(set(best_span_list), key=lambda x: x[0])\n",
    "    merged = []\n",
    "    for interval in intervals:\n",
    "        if not merged or merged[-1][1] < interval[0]:\n",
    "            merged.append(interval)\n",
    "        else:\n",
    "            merged[-1] = (merged[-1][0], max(merged[-1][1], interval[1]))\n",
    "\n",
    "    print(merged)\n",
    "    evidence_sentences = [' '.join(sentences[merged[i][0]:merged[i][1]]) for i in range(len(merged))]\n",
    "    return ' '.join(evidence_sentences)\n",
    "\n",
    "\n",
    "find_best_span_for_all_evidences(reference_edges,\n",
    "                                 claim_reference_edge_map,\n",
    "                                 reference_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [\n",
    "    (1, 2), (9, 10), (13, 14), (15, 16), (13, 14),\n",
    "    (1, 2), (9, 10), (13, 14), (15, 16), (13, 14),\n",
    "    (1, 2), (9, 10), (13, 14), (15, 16), (13, 14),\n",
    "    (5, 6), (9, 10)\n",
    "]\n",
    "\n",
    "# Step 1: Sort and remove duplicates\n",
    "intervals = sorted(set(intervals), key=lambda x: x[0])\n",
    "\n",
    "# Step 2: Merge intervals\n",
    "merged = []\n",
    "for interval in intervals:\n",
    "    if not merged or merged[-1][1] < interval[0]:\n",
    "        merged.append(interval)\n",
    "    else:\n",
    "        merged[-1] = (merged[-1][0], max(merged[-1][1], interval[1]))\n",
    "\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_reference_edge_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TASKNAMES = [\"subtask1\", \"subtask2\"]\n",
    "BATCHNAMES = [\"batch1\", \"batch2\", \"batch3\"]\n",
    "\n",
    "dataset_loader = dict()\n",
    "\n",
    "for taskname in TASKNAMES:\n",
    "    for batchname in BATCHNAMES:\n",
    "        print(f\"Processing {taskname} for {batchname}\")\n",
    "        prefix = \"dataset/\"\n",
    "        # Load the data\n",
    "        data = pd.read_csv(f\"{prefix}/{taskname}_train_{batchname}.csv\")\n",
    "        dataset_loader[f\"{taskname}_{batchname}\"] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_loader[\"subtask1_batch2\"].answer.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_loader[\"subtask1_batch2\"].claim.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_reference, edges_reference = read_knowledge_graph_from_pickle(files[0])\n",
    "nodes_claim, edges_claim = read_knowledge_graph_from_pickle(files[1])\n",
    "for key in nodes_claim.keys():\n",
    "    if key in nodes_reference.keys():\n",
    "        print(f\"`{key}`\")\n",
    "    else:\n",
    "        print(f\"Key `{key}` not found in nodes_reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_src, key_tgt in edges_claim.keys():\n",
    "    print(f\"`{key_src}` -> `{key_tgt}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geminillm import gemini_complete_if_cache\n",
    "from json_kv_iml import JsonKVStorage\n",
    "from operationCheatSheet import preprocessing_claim\n",
    "from openaillm import openai_embed\n",
    "\n",
    "LLM_MODEL_NAME = \"gemini-2.5-flash-preview-05-20\"\n",
    "\n",
    "\n",
    "kv_global_config = {\n",
    "    \"working_dir\": \"/tmp\",\n",
    "    \"llm_model_name\": LLM_MODEL_NAME,\n",
    "    \"embedding_batch_num\": 64,  # or another integer suitable for your setup\n",
    "    \"vector_db_storage_cls_kwargs\": {\n",
    "        \"cosine_better_than_threshold\": 0.2  # or another float threshold you want\n",
    "    },\n",
    "    \"base_url\": \"https://api.openai.com/v1\",\n",
    "}\n",
    "\n",
    "llm_cache = JsonKVStorage(namespace=\"llm_cache\", global_config=kv_global_config, embedding_func=openai_embed)\n",
    "    \n",
    "async def llm_wrapper(prompt, history_messages=None, max_tokens=None, **kwargs):\n",
    "    if history_messages is None:\n",
    "        history_messages = []\n",
    "\n",
    "    # Use Google GenAI\n",
    "    return await gemini_complete_if_cache(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        prompt=prompt,\n",
    "        history_messages=history_messages,\n",
    "        hashing_kv=llm_cache,\n",
    "        temperature=0.2,\n",
    "        max_tokens=max_tokens or 1024,\n",
    "    )\n",
    "# answer = dataset_loader[\"subtask1_batch2\"].answer.tolist()[0]\n",
    "# claim = dataset_loader[\"subtask1_batch2\"].claim.tolist()[0]\n",
    "# response = await preprocessing_claim(claim, answer, llm_wrapper, llm_cache, 1024, [])\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answer_batch_2 = dataset_loader[\"subtask1_batch2\"].answer.tolist()\n",
    "all_claim_batch_2 = dataset_loader[\"subtask1_batch2\"].claim.tolist()\n",
    "\n",
    "preprocess_path = \"dataset/subtask1_train_batch2_preprocess.csv\"\n",
    "df = pd.read_csv(preprocess_path)\n",
    "for index, (answer, claim) in enumerate(zip(all_answer_batch_2, all_claim_batch_2)):\n",
    "    if index % 200 == 0:\n",
    "        print(f\"Processing index {index}\")\n",
    "    try:\n",
    "        response = await preprocessing_claim(claim, answer, llm_wrapper, llm_cache, 1024, [])\n",
    "        if response is None:\n",
    "            print(f\"Index {index} has no response\")\n",
    "            continue\n",
    "        else:\n",
    "            df.at[index, \"claim\"] = response\n",
    "    except Exception as e:\n",
    "        print(f\"Index {index} has error\")\n",
    "        print(e)\n",
    "\n",
    "df.to_csv(preprocess_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretty_prompt_compare import PrettyCompare\n",
    "\n",
    "pretty_compare = PrettyCompare(compare_response=True, target=[\"they\", \"They\", \n",
    "                                                              \"them\", \"Them\",\n",
    "                                                              \"their\", \"Their\",\n",
    "                                                              \"theirs\", \"Theirs\",\n",
    "                                                              \"them\", \"Them\",\n",
    "                                                              \"these\", \"These\",\n",
    "                                                              \"those\", \"Those\",\n",
    "                                                              \"this\", \"This\",\n",
    "                                                              \"that\", \"That\",\n",
    "                                                              \"these\", \"These\",\n",
    "                                                              \"those\", \"Those\",\n",
    "                                                              \"this\", \"This\"])\n",
    "\n",
    "claim_processed = df.claim.tolist()\n",
    "all_claim_batch_2 = dataset_loader[\"subtask1_batch2\"].claim.tolist()\n",
    "\n",
    "index = 21\n",
    "\n",
    "all_claim_batch_2[index] |pretty_compare| claim_processed[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[0, \"claim\"] = \"2\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_and_rename_newest_files_by_number():\n",
    "    \"\"\"\n",
    "    Find the newest claim_batch2 files for each number (0-999) and rename them to claim_test format\n",
    "    \"\"\"\n",
    "    outputs_dir = \"outputs\"\n",
    "    \n",
    "    # Pattern to match: claim_batch2_{number}_result_{timestamp}.pkl\n",
    "    pattern = r'claim_batch2_(\\d+)_result_(\\d{8}_\\d{6})\\.(pkl|txt)'\n",
    "    \n",
    "    # Dictionary to store files by number, then by timestamp\n",
    "    files_by_number = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Scan all files in outputs directory\n",
    "    for filename in os.listdir(outputs_dir):\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            timestamp = match.group(2)\n",
    "            extension = match.group(3)\n",
    "            \n",
    "            files_by_number[number][timestamp].append({\n",
    "                'filename': filename,\n",
    "                'number': number,\n",
    "                'timestamp': timestamp,\n",
    "                'extension': extension,\n",
    "                'full_path': os.path.join(outputs_dir, filename)\n",
    "            })\n",
    "    \n",
    "    if not files_by_number:\n",
    "        print(\"No files matching the pattern found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found files for {len(files_by_number)} different numbers\")\n",
    "    \n",
    "    # For each number, find the newest timestamp and rename those files\n",
    "    renamed_count = 0\n",
    "    numbers_processed = []\n",
    "    \n",
    "    for number in sorted(files_by_number.keys(), key=int):\n",
    "        timestamps_for_number = files_by_number[number]\n",
    "        \n",
    "        # Find the newest timestamp for this number\n",
    "        newest_timestamp = max(timestamps_for_number.keys())\n",
    "        newest_files = timestamps_for_number[newest_timestamp]\n",
    "        \n",
    "        print(f\"\\nNumber {number}: Found {len(newest_files)} files with newest timestamp {newest_timestamp}\")\n",
    "        \n",
    "        # Rename the files for this number\n",
    "        for file_info in newest_files:\n",
    "            old_path = file_info['full_path']\n",
    "            \n",
    "            # Create new filename: claim_test_result_{number}_{timestamp}.{extension}\n",
    "            new_filename = f\"claim_test_result_{file_info['number']}_{file_info['timestamp']}.{file_info['extension']}\"\n",
    "            new_path = os.path.join(outputs_dir, new_filename)\n",
    "            \n",
    "            try:\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"  Renamed: {file_info['filename']} -> {new_filename}\")\n",
    "                renamed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error renaming {file_info['filename']}: {e}\")\n",
    "        \n",
    "        numbers_processed.append(number)\n",
    "    \n",
    "    print(f\"\\nSuccessfully renamed {renamed_count} files for {len(numbers_processed)} different numbers!\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\nProcessed numbers: {', '.join(sorted(numbers_processed, key=int))}\")\n",
    "    \n",
    "    # Show which numbers from 0-999 are missing\n",
    "    all_numbers = set(str(i) for i in range(1000))\n",
    "    found_numbers = set(files_by_number.keys())\n",
    "    missing_numbers = all_numbers - found_numbers\n",
    "    \n",
    "    if missing_numbers:\n",
    "        missing_sorted = sorted(missing_numbers, key=int)\n",
    "        print(f\"\\nNumbers with no files found: {', '.join(missing_sorted[:20])}\")\n",
    "        if len(missing_numbers) > 20:\n",
    "            print(f\"... and {len(missing_numbers) - 20} more\")\n",
    "\n",
    "\n",
    "find_and_rename_newest_files_by_number()\n",
    "\n",
    "\n",
    "find_and_rename_newest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_latest_result_by_prefix\n",
    "index  = 0\n",
    "claim_nodes, claim_edges = get_latest_result_by_prefix(f\"claim_test_result_{index}_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def rename_claim_test_files():\n",
    "    \"\"\"\n",
    "    Rename files from claim_test_result_{number}_{timestamp}.{ext} \n",
    "    to claim_test_{number}_result_{timestamp}.{ext}\n",
    "    \"\"\"\n",
    "    outputs_dir = \"outputs\"\n",
    "    \n",
    "    # Pattern to match: claim_test_result_{number}_{timestamp}.{extension}\n",
    "    pattern = r'claim_test_result_(\\d+)_(\\d{8}_\\d{6})\\.(pkl|txt)'\n",
    "    \n",
    "    # Get all files in outputs directory\n",
    "    files_to_rename = []\n",
    "    \n",
    "    for filename in os.listdir(outputs_dir):\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            timestamp = match.group(2)\n",
    "            extension = match.group(3)\n",
    "            \n",
    "            # Create new filename: claim_test_{number}_result_{timestamp}.{extension}\n",
    "            new_filename = f\"claim_test_{number}_result_{timestamp}.{extension}\"\n",
    "            \n",
    "            files_to_rename.append({\n",
    "                'old_filename': filename,\n",
    "                'new_filename': new_filename,\n",
    "                'old_path': os.path.join(outputs_dir, filename),\n",
    "                'new_path': os.path.join(outputs_dir, new_filename)\n",
    "            })\n",
    "    \n",
    "    if not files_to_rename:\n",
    "        print(\"No files matching the pattern 'claim_test_result_*' found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(files_to_rename)} files to rename:\")\n",
    "    for file_info in files_to_rename:\n",
    "        print(f\"  {file_info['old_filename']} -> {file_info['new_filename']}\")\n",
    "    \n",
    "    print(f\"\\nProceeding with renaming {len(files_to_rename)} files...\")\n",
    "    \n",
    "    # Perform the renaming\n",
    "    renamed_count = 0\n",
    "    for file_info in files_to_rename:\n",
    "        try:\n",
    "            os.rename(file_info['old_path'], file_info['new_path'])\n",
    "            print(f\"✓ Renamed: {file_info['old_filename']} -> {file_info['new_filename']}\")\n",
    "            renamed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error renaming {file_info['old_filename']}: {e}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully renamed {renamed_count} out of {len(files_to_rename)} files!\")\n",
    "\n",
    "rename_claim_test_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_latest_result_by_prefix\n",
    "\n",
    "index = 1\n",
    "claim_nodes, claim_edges = get_latest_result_by_prefix(f\"claim_test_{index}_\")\n",
    "\n",
    "claim_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from kg_compare import get_src_tgt_dict, get_map_src_tgt_dict, find_best_span\n",
    "\n",
    "from kg_compare import compare_knowledge_graphs_similarity, find_best_span_for_all_evidences\n",
    "from utils import get_latest_result_by_prefix\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "all_responses = []\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from geminillm import gemini_complete_if_cache\n",
    "from json_kv_iml import JsonKVStorage\n",
    "from operationCheatSheet import preprocessing_claim\n",
    "from openaillm import openai_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geminillm import gemini_complete_if_cache\n",
    "from json_kv_iml import JsonKVStorage\n",
    "from operationCheatSheet import preprocessing_claim\n",
    "from openaillm import openai_embed\n",
    "\n",
    "LLM_MODEL_NAME = \"gemini-2.5-flash-preview-05-20\"\n",
    "\n",
    "\n",
    "kv_global_config = {\n",
    "    \"working_dir\": \"/tmp\",\n",
    "    \"llm_model_name\": LLM_MODEL_NAME,\n",
    "    \"embedding_batch_num\": 64,  # or another integer suitable for your setup\n",
    "    \"vector_db_storage_cls_kwargs\": {\n",
    "        \"cosine_better_than_threshold\": 0.2  # or another float threshold you want\n",
    "    },\n",
    "    \"base_url\": \"https://api.openai.com/v1\",\n",
    "}\n",
    "\n",
    "llm_cache = JsonKVStorage(namespace=\"llm_cache\", global_config=kv_global_config, embedding_func=openai_embed)\n",
    "    \n",
    "async def llm_wrapper(prompt, history_messages=None, max_tokens=None, **kwargs):\n",
    "    if history_messages is None:\n",
    "        history_messages = []\n",
    "\n",
    "    # Use Google GenAI\n",
    "    return await gemini_complete_if_cache(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        prompt=prompt,\n",
    "        history_messages=history_messages,\n",
    "        hashing_kv=llm_cache,\n",
    "        temperature=0.2,\n",
    "        max_tokens=max_tokens or 10000,\n",
    "    )\n",
    "# answer = dataset_loader[\"subtask1_batch2\"].answer.tolist()[0]\n",
    "# claim = dataset_loader[\"subtask1_batch2\"].claim.tolist()[0]\n",
    "# response = await preprocessing_claim(claim, answer, llm_wrapper, llm_cache, 1024, [])\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subtask1 for test\n",
      "Processing subtask1 for test_preprocess\n",
      "Processing subtask1 for train_batch2\n",
      "Processing subtask1 for train_batch2_preprocess\n",
      "Processing subtask2 for test\n",
      "Processing subtask2 for test_preprocess\n",
      "Processing subtask2 for train_batch2\n",
      "Processing subtask2 for train_batch2_preprocess\n"
     ]
    }
   ],
   "source": [
    "TASKNAMES = [\"subtask1\", \"subtask2\"]\n",
    "TYPES = [\"test\", \"test_preprocess\", \"train_batch2\", \"train_batch2_preprocess\"]\n",
    "\n",
    "dataset_loader = dict()\n",
    "\n",
    "for taskname in TASKNAMES:\n",
    "    for type in TYPES:\n",
    "        print(f\"Processing {taskname} for {type}\")\n",
    "        prefix = \"dataset\"\n",
    "        # Load the data\n",
    "        \n",
    "        if taskname == \"subtask2\" and type == \"test_preprocess\":\n",
    "            continue\n",
    "        if taskname == \"subtask2\" and type == \"train_batch2_preprocess\":\n",
    "            continue\n",
    "        data = pd.read_csv(f\"{prefix}/{taskname}_{type}.csv\")\n",
    "        dataset_loader[f\"{taskname}_{type}\"] = data\n",
    "\n",
    "dataset_loader[\"subtask1_test\"][\"claim\"] = dataset_loader[\"subtask1_test_preprocess\"][\"claim\"]\n",
    "dataset_loader[\"subtask1_train_batch2\"][\"claim\"] = dataset_loader[\"subtask1_train_batch2_preprocess\"][\"claim\"]\n",
    "\n",
    "output_file = \"outputs/response_test.txt\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL_NAME = \"gemini-2.5-flash-preview-05-20\"\n",
    "def write_prediction_to_file(index, row_id, prediction, output_file=\"outputs/response_test.txt\"):\n",
    "    \"\"\"\n",
    "    Write a line with row ID and prediction to the output file\n",
    "    \n",
    "    Args:\n",
    "        row_id: The ID/index of the row being processed\n",
    "        prediction: The prediction result\n",
    "        output_file: Path to the output file\n",
    "    \"\"\"\n",
    "    # Ensure outputs directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Write the prediction with row ID to file\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"{index}, {row_id}, {prediction}\\n\")\n",
    "\n",
    "async def llm_wrapper(prompt, history_messages=None, max_tokens=None, **kwargs):\n",
    "    if history_messages is None:\n",
    "        history_messages = []\n",
    "\n",
    "\n",
    "    # Use Google GenAI\n",
    "    return await gemini_complete_if_cache(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        prompt=prompt,\n",
    "        history_messages=history_messages,\n",
    "        hashing_kv=llm_cache,\n",
    "        temperature=0.2,\n",
    "        max_tokens=max_tokens or 1024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "folder_path = \"outputs/kaggle_results/\"\n",
    "\n",
    "prefix = \"response_clean\"\n",
    "\n",
    "\n",
    "# Step 1: Set up folder and file pattern\n",
    "pattern = os.path.join(folder_path, f\"{prefix}*.csv\")\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Step 2: Identify main file and other files\n",
    "main_file = [f for f in csv_files if 'jun26_1158' in f][0]\n",
    "other_files = [f for f in csv_files if f != main_file]\n",
    "\n",
    "# Step 2: Read and split \"ID\\tlabel\" column into two columns: ID, label\n",
    "dfs = []\n",
    "def load_and_split(file):\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "dfs = [load_and_split(file) for file in csv_files]\n",
    "main_df = load_and_split(main_file)\n",
    "other_dfs = [load_and_split(file) for file in other_files]\n",
    "\n",
    "# Step 4: Concatenate other files for majority voting\n",
    "other_data = pd.concat(other_dfs)\n",
    "\n",
    "# Step 5: Group by ID and get all labels (for checking if all disagree)\n",
    "other_labels = other_data.groupby(\"ID\")[\"label\"].apply(list)\n",
    "\n",
    "# Step 6: Merge with main_df to align labels\n",
    "main_df = main_df.merge(other_labels, on=\"ID\", how=\"left\")  # Now has 'label_x' and 'label_y'\n",
    "\n",
    "# Rename for clarity\n",
    "main_df = main_df.rename(columns={\"label_x\": \"main_label\", \"label_y\": \"other_labels\"})\n",
    "\n",
    "# Step 7: Apply conditional rule\n",
    "def decide_label(row):\n",
    "    main_label = row['main_label']\n",
    "    other_votes = row['other_labels']\n",
    "    if other_votes and all(label != main_label for label in other_votes):\n",
    "        # Override with majority vote\n",
    "        return Counter(other_votes).most_common(1)[0][0]\n",
    "    else:\n",
    "        return main_label\n",
    "\n",
    "main_df['final_label'] = main_df.apply(decide_label, axis=1)\n",
    "\n",
    "# Step 8: Build final result\n",
    "final_df = main_df[['ID', 'final_label']].rename(columns={'final_label': 'label'})\n",
    "best_df = pd.read_csv(\"outputs/kaggle_results/response_clean_jun26_1158.csv\")\n",
    "unver_list = [index for index in range(len(best_df)) if best_df.at[index, \"label\"] == \"unver\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items loaded: 473\n",
      "\n",
      "Content for line number '994':\n",
      "Reference [4] focuses on the diagnosis of tooth surface damage faults in gears using induction machine electrical signature analysis,  emphasizing the crucial role of gear condition monitoring in mechanical power transmission. Additionally,  Reference [2] notes that for applications like compressors,  the interaction between the rotor and the surrounding medium can be significant.\n",
      "\n",
      "Content for line number '6':\n",
      "Unpleasant odor is identified as one of the major problems encountered in patients with chronic wounds.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# This script demonstrates how to read and parse the summary_test.txt file\n",
    "# and store the data in a dictionary for easy access by the line number identifier from the file.\n",
    "\n",
    "file_path = 'outputs/summary/summary_test.txt'\n",
    "data_by_linenum = {}\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            \n",
    "            # The first column is the line number identifier\n",
    "            line_number = row[0].strip()\n",
    "            item_id = row[1].strip()\n",
    "            content = ', '.join(row[2:]).strip()\n",
    "            \n",
    "            # Store the data in a dictionary with the line number as the key.\n",
    "            data_by_linenum[line_number] = {\n",
    "                'id': item_id,\n",
    "                'content': content\n",
    "            }\n",
    "\n",
    "    # Now you can easily access data by its line number.\n",
    "    print(f\"Total items loaded: {len(data_by_linenum)}\\n\")\n",
    "\n",
    "    # --- Example Usage ---\n",
    "    # Let's get the content for the line number '994'\n",
    "    target_line = '994'\n",
    "    \n",
    "    if target_line in data_by_linenum:\n",
    "        print(f\"Content for line number '{target_line}':\")\n",
    "        print(data_by_linenum[target_line]['content'])\n",
    "    else:\n",
    "        print(f\"Could not find data for line number '{target_line}'.\")\n",
    "        \n",
    "    # Another example for line '6'\n",
    "    target_line_2 = '6'\n",
    "    if target_line_2 in data_by_linenum:\n",
    "        print(f\"\\nContent for line number '{target_line_2}':\")\n",
    "        print(data_by_linenum[target_line_2]['content'])\n",
    "    else:\n",
    "        print(f\"\\nCould not find data for line number '{target_line_2}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 0\n",
      "Processing index 50\n",
      "Processing index 100\n",
      "Processing index 150\n",
      "Processing index 200\n",
      "Processing index 250\n",
      "Processing index 300\n",
      "Processing index 350\n",
      "Processing index 400\n",
      "Processing index 450\n",
      "Processing index 500\n",
      "Processing index 550\n",
      "Processing index 600\n",
      "Processing index 650\n",
      "Processing index 700\n",
      "Processing index 750\n",
      "Processing index 800\n",
      "Processing index 850\n",
      "Processing index 900\n",
      "Processing index 950\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset_loader[\"subtask1_test\"]\n",
    "\n",
    "pickle_prefix = \"test\"\n",
    "# pickle_prefix = \"batch2\"\n",
    "\n",
    "for index in range(len(test_data)):\n",
    "    if index % 50 == 0:\n",
    "        print(f\"Processing index {index}\")\n",
    "\n",
    "    row_id = test_data.at[index, \"ID\"]\n",
    "    \n",
    "    if index not in unver_list:\n",
    "        response = best_df.at[index, \"label\"]\n",
    "        all_responses.append(response)\n",
    "        write_prediction_to_file(index, row_id, response)\n",
    "        continue\n",
    "    \n",
    "    claim_content = test_data.at[index, \"claim\"]\n",
    "\n",
    "    claim_nodes, claim_edges = await get_latest_result_by_prefix(f\"claim_{pickle_prefix}_{index}_\")\n",
    "    reference_nodes, reference_edges = await get_latest_result_by_prefix(f\"reference_{pickle_prefix}_{index}_\")\n",
    "    reference_content = test_data.at[index, \"reference\"]\n",
    "\n",
    "    claim_node_list = list(claim_nodes.keys())\n",
    "    reference_node_list = list(reference_nodes.keys())\n",
    "\n",
    "    if claim_node_list == [] or reference_node_list == []:\n",
    "        if str(index) in data_by_linenum:\n",
    "            evidence_text = data_by_linenum[str(index)][\"content\"]\n",
    "        else:\n",
    "            evidence_text = reference_content\n",
    "    else:\n",
    "        similarity_matrix = compare_knowledge_graphs_similarity(claim_node_list,\n",
    "                                                                reference_node_list,\n",
    "                                                                if_plot=False)\n",
    "        src_tgt_dict = get_src_tgt_dict(similarity_matrix, claim_node_list, reference_node_list)\n",
    "\n",
    "        claim_reference_edge_map = get_map_src_tgt_dict(src_tgt_dict, claim_edges)\n",
    "\n",
    "        evidence_text = find_best_span_for_all_evidences(reference_edges,\n",
    "                                    claim_reference_edge_map,\n",
    "                                    reference_content)\n",
    "    \n",
    "    if evidence_text == \"\":\n",
    "        if str(index) in data_by_linenum:\n",
    "            evidence_text = data_by_linenum[str(index)][\"content\"]\n",
    "        else:\n",
    "            evidence_text = reference_content\n",
    "    \n",
    "    classification_prompt = f\"\"\"\n",
    "    ### INSTRUCTION ###\n",
    "    You are an expert fact-checking AI. Your task is to meticulously analyze a claim against a piece of evidence and classify their relationship. Follow these steps:\n",
    "\n",
    "    1.  **Analyze the Claim**: Identify the core assertion being made in the claim.\n",
    "    2.  **Analyze the Evidence**: Understand what information the evidence provides and what it does not.\n",
    "    3.  **Compare**: Compare the claim's core assertion to the information in the evidence.\n",
    "    4.  **Classify**: Based on your comparison, classify the claim into ONE of the three categories defined below.\n",
    "\n",
    "    ### CATEGORY DEFINITIONS ###\n",
    "\n",
    "    1.  **Entailment**: Choose this if the evidence **directly supports, paraphrases, or is a logical consequence of** the claim. The evidence doesn't need to be a word-for-word match. If the core assertion of the claim is validated by the evidence, it is Entailment.\n",
    "\n",
    "    2.  **Contradiction**: Choose this if the evidence **directly contradicts** the claim. This includes claims that **exaggerate or overstate** what the evidence says (e.g., claim says \"will completely solve\" but evidence says \"will help assist\").\n",
    "\n",
    "    3.  **Unverifiable**: Choose this ONLY if a **key piece of the claim's main assertion is not mentioned** in the evidence, making it impossible to confirm or deny. Do NOT use this just because the wording is different. If the evidence supports the *gist* of the claim, it is likely Entailment, not Unverifiable.\n",
    "\n",
    "    ### EXAMPLES ###\n",
    "\n",
    "    **Example 1 (Clear Entailment):**\n",
    "    Claim: Machine learning in VR can create intelligent agents that adapt to user behavior for a more personalized experience.\n",
    "    Evidence: A novel approach for automated navigation and searching is proposed by incorporating machine learning in virtual reality. An intelligent virtual agent learns objects of interest along with the paths followed for navigation.\n",
    "    Classification: Entailment\n",
    "\n",
    "    **Example 2 (Unverifiable - Key Info Missing):**\n",
    "    Claim: AI-driven VR environments are used for safe training in healthcare and engineering.\n",
    "    Evidence: Virtual Reality frameworks are a promising innovation relevant in areas like preparing test systems, therapeutic and human services, training, and the stimulation industry.\n",
    "    *Reasoning*: The evidence mentions VR for training and healthcare, but it crucially omits any mention of \"AI-driven\" or \"engineering\". A key component of the claim is missing.\n",
    "    Classification: Unverifiable\n",
    "\n",
    "    **Example 3 (Contradiction - Overstatement):**\n",
    "    Claim: Chatbots can assist students in choosing a major, which may **completely eliminate** the stress and confusion associated with this decision.\n",
    "    Evidence: The making of this chatbot aims to **assist** prospective students in determining majors according to their personality. The majors' questionnaire is one way to **assist** students in recommending what majors they should have.\n",
    "    *Reasoning*: The claim's use of \"completely eliminate\" is a massive exaggeration of the evidence's \"assist\". This is a contradiction.\n",
    "    Classification: Contradiction\n",
    "\n",
    "    **Example 4 (Entailment - Paraphrasing/Inference):**\n",
    "    Claim: The system makes it easier for users to navigate virtual spaces.\n",
    "    Evidence: A novel approach for automated navigation and searching is proposed by incorporating machine learning in virtual reality.\n",
    "    *Reasoning*: \"Easier to navigate\" is a reasonable summary of \"automated navigation and searching\". This is Entailment.\n",
    "    Classification: Entailment\n",
    "\n",
    "    ### YOUR TASK ###\n",
    "    Now, perform your step-by-step analysis on the following claim and evidence, then provide your final classification.\n",
    "\n",
    "    Claim:\n",
    "    {claim_content}\n",
    "\n",
    "    Evidence:\n",
    "    {evidence_text}\n",
    "\n",
    "    ### OUTPUT REQUIREMENTS ###\n",
    "    - You MUST respond with ONLY one of the three words: \"Entailment\", \"Contradiction\", or \"Unverifiable\".\n",
    "    - Your entire response must be EXACTLY one of these three words.\n",
    "    - Do NOT provide explanations, justifications, or any additional text in your final output.\n",
    "\n",
    "    Classification:\n",
    "    \"\"\"\n",
    "\n",
    "    response = await llm_wrapper(classification_prompt)\n",
    "    all_responses.append(response)\n",
    "    write_prediction_to_file(index, row_id, response)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original file\n",
    "with open(\"outputs/response_test.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split each line and extract ID and label\n",
    "full_data = [line.strip().split(\", \")[1:] for line in lines]  # skip the first field (index)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(full_data, columns=[\"ID\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].replace({\n",
    "    \"Unverifiable\": \"unver\",\n",
    "    \"Contradiction\": \"contra\",\n",
    "    \"Entailment\": \"entail\"\n",
    "})\n",
    "\n",
    "# Drop exact duplicate rows (same ID and label)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df.to_csv(\"outputs/kaggle_results/response_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = dataset_loader[\"subtask1_train_batch2\"][:100].copy()\n",
    "results_df[\"true_label\"] = results_df[\"label\"]\n",
    "results_df[\"predicted_label\"]=  df.label[-100:].values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "true_labels = results_df['true_label'].tolist()\n",
    "predicted_labels = results_df['predicted_label'].tolist()\n",
    "\n",
    "# Calculate weighted F1 score (main evaluation metric)\n",
    "weighted_f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"\\n=== EVALUATION RESULTS ===\")\n",
    "print(f\"Total samples processed: {len(results_df)}\")\n",
    "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (results_df['true_label'] == results_df['predicted_label']).mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(f\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "# Show results by batch\n",
    "print(f\"\\n=== RESULTS BY BATCH ===\")\n",
    "for batch in results_df['batch'].unique():\n",
    "    batch_results = results_df[results_df['batch'] == batch]\n",
    "    batch_accuracy = (batch_results['true_label'] == batch_results['predicted_label']).mean()\n",
    "    batch_f1 = f1_score(batch_results['true_label'], batch_results['predicted_label'], average='weighted')\n",
    "    print(f\"{batch}: Accuracy={batch_accuracy:.4f}, Weighted F1={batch_f1:.4f}\")\n",
    "\n",
    "# Calculate F1 scores for batch2 and batch3 separately\n",
    "print(f\"\\n=== INDIVIDUAL BATCH F1 SCORES ===\")\n",
    "if 'batch2' in results_df['batch'].values:\n",
    "    batch2_results = results_df[results_df['batch'] == 'batch2']\n",
    "    batch2_f1 = f1_score(batch2_results['true_label'], batch2_results['predicted_label'], average='weighted')\n",
    "    print(f\"Batch2 Weighted F1 Score: {batch2_f1:.4f}\")\n",
    "\n",
    "if 'batch3' in results_df['batch'].values:\n",
    "    batch3_results = results_df[results_df['batch'] == 'batch3']\n",
    "    batch3_f1 = f1_score(batch3_results['true_label'], batch3_results['predicted_label'], average='weighted')\n",
    "    print(f\"Batch3 Weighted F1 Score: {batch3_f1:.4f}\")\n",
    "\n",
    "# Display first few results\n",
    "print(f\"\\n=== SAMPLE RESULTS ===\")\n",
    "print(results_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
