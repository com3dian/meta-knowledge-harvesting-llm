{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TASKNAMES = [\"subtask1\", \"subtask2\"]\n",
    "TYPES = [\"test\", \"test_preprocess\"]\n",
    "\n",
    "dataset_loader = dict()\n",
    "\n",
    "for taskname in TASKNAMES:\n",
    "    for type in TYPES:\n",
    "        print(f\"Processing {taskname} for {type}\")\n",
    "        prefix = \"dataset\"\n",
    "        # Load the data\n",
    "        \n",
    "        if taskname == \"subtask2\" and type == \"test_preprocess\":\n",
    "            continue\n",
    "        data = pd.read_csv(f\"{prefix}/{taskname}_{type}.csv\")\n",
    "        dataset_loader[f\"{taskname}_{type}\"] = data\n",
    "\n",
    "dataset_loader[\"subtask1_test\"][\"claim\"] = dataset_loader[\"subtask1_test_preprocess\"][\"claim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from rapidfuzz import fuzz\n",
    "from utils import get_latest_result_by_prefix\n",
    "\n",
    "from kg_compare import get_src_tgt_dict, get_map_src_tgt_dict, compare_knowledge_graphs_similarity\n",
    "test_data = dataset_loader[\"subtask1_test\"]\n",
    "\n",
    "index = 6\n",
    "claim_nodes, claim_edges = get_latest_result_by_prefix(f\"claim_test_{index}_\")\n",
    "reference_nodes, reference_edges = get_latest_result_by_prefix(f\"reference_test_{index}_\")\n",
    "reference_content = test_data.at[index, \"reference\"]\n",
    "\n",
    "claim_node_list = list(claim_nodes.keys())\n",
    "reference_node_list = list(reference_nodes.keys())\n",
    "\n",
    "if claim_node_list == [] or reference_node_list == []:\n",
    "    print(\"Claim or reference is empty\")\n",
    "    \n",
    "    \n",
    "similarity_matrix = compare_knowledge_graphs_similarity(claim_node_list,\n",
    "                                                        reference_node_list,\n",
    "                                                        if_plot=False)\n",
    "src_tgt_dict = get_src_tgt_dict(similarity_matrix, claim_node_list, reference_node_list)\n",
    "\n",
    "claim_reference_edge_map = get_map_src_tgt_dict(src_tgt_dict, claim_edges)\n",
    "\n",
    "def find_best_span_for_all_evidences(reference_edges,\n",
    "                                     claim_reference_edge_map,\n",
    "                                     paragraph,\n",
    "                                     max_window_size=5):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "\n",
    "    best_span_list = []\n",
    "    for _, reference_edge_list in claim_reference_edge_map.items():\n",
    "        evidence_edges = set(reference_edge_list) & set(reference_edges.keys())\n",
    "\n",
    "        for evidence_edge in evidence_edges:\n",
    "            reference_edge = reference_edges[evidence_edge]\n",
    "\n",
    "            best_score = -1\n",
    "            best_span = None\n",
    "\n",
    "            for window_size in range(1, min(max_window_size, len(sentences)) + 1):\n",
    "                for i in range(len(sentences) - window_size + 1):\n",
    "                    window = ' '.join(sentences[i:i+window_size])\n",
    "                    score = fuzz.ratio(reference_edge[0][\"description\"], window)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_span = (i, i+window_size)\n",
    "            best_span_list.append(best_span)\n",
    "\n",
    "\n",
    "    intervals = sorted(set(best_span_list), key=lambda x: x[0])\n",
    "    merged = []\n",
    "    for interval in intervals:\n",
    "        if not merged or merged[-1][1] < interval[0]:\n",
    "            merged.append(interval)\n",
    "        else:\n",
    "            merged[-1] = (merged[-1][0], max(merged[-1][1], interval[1]))\n",
    "\n",
    "    print(merged)\n",
    "    evidence_sentences = [' '.join(sentences[merged[i][0]:merged[i][1]]) for i in range(len(merged))]\n",
    "    return ' '.join(evidence_sentences)\n",
    "\n",
    "\n",
    "find_best_span_for_all_evidences(reference_edges,\n",
    "                                 claim_reference_edge_map,\n",
    "                                 reference_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [\n",
    "    (1, 2), (9, 10), (13, 14), (15, 16), (13, 14),\n",
    "    (1, 2), (9, 10), (13, 14), (15, 16), (13, 14),\n",
    "    (1, 2), (9, 10), (13, 14), (15, 16), (13, 14),\n",
    "    (5, 6), (9, 10)\n",
    "]\n",
    "\n",
    "# Step 1: Sort and remove duplicates\n",
    "intervals = sorted(set(intervals), key=lambda x: x[0])\n",
    "\n",
    "# Step 2: Merge intervals\n",
    "merged = []\n",
    "for interval in intervals:\n",
    "    if not merged or merged[-1][1] < interval[0]:\n",
    "        merged.append(interval)\n",
    "    else:\n",
    "        merged[-1] = (merged[-1][0], max(merged[-1][1], interval[1]))\n",
    "\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_reference_edge_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TASKNAMES = [\"subtask1\", \"subtask2\"]\n",
    "BATCHNAMES = [\"batch1\", \"batch2\", \"batch3\"]\n",
    "\n",
    "dataset_loader = dict()\n",
    "\n",
    "for taskname in TASKNAMES:\n",
    "    for batchname in BATCHNAMES:\n",
    "        print(f\"Processing {taskname} for {batchname}\")\n",
    "        prefix = \"dataset/\"\n",
    "        # Load the data\n",
    "        data = pd.read_csv(f\"{prefix}/{taskname}_train_{batchname}.csv\")\n",
    "        dataset_loader[f\"{taskname}_{batchname}\"] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_loader[\"subtask1_batch2\"].answer.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'values': value_list,\n",
    "    'labels': label_list\n",
    "})\n",
    "\n",
    "# Ensure values are numeric - convert to float and handle any non-numeric values\n",
    "plot_df['values'] = pd.to_numeric(plot_df['values'], errors='coerce')\n",
    "\n",
    "# Remove any rows with NaN values (from failed conversions)\n",
    "plot_df = plot_df.dropna()\n",
    "\n",
    "print(f\"Data info:\")\n",
    "print(f\"Total samples: {len(plot_df)}\")\n",
    "print(f\"Value range: [{plot_df['values'].min():.3f}, {plot_df['values'].max():.3f}]\")\n",
    "print(f\"Unique labels: {sorted(plot_df['labels'].unique())}\")\n",
    "print()\n",
    "\n",
    "# Set up the plot style\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create density plots for each label\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Overlapping density distributions\n",
    "for label in sorted(plot_df['labels'].unique()):\n",
    "    subset = plot_df[plot_df['labels'] == label]\n",
    "    if len(subset) > 0:  # Only plot if there's data\n",
    "        sns.kdeplot(data=subset, x='values', label=label, ax=ax1, alpha=0.7, fill=True)\n",
    "\n",
    "ax1.set_title('Density Distribution of Model Scores by Label', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Model Score Values', fontsize=12)\n",
    "ax1.set_ylabel('Density', fontsize=12)\n",
    "ax1.legend(title='True Labels', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Stacked histogram for comparison\n",
    "bins = np.linspace(plot_df['values'].min(), plot_df['values'].max(), 30)\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "for i, label in enumerate(sorted(plot_df['labels'].unique())):\n",
    "    subset = plot_df[plot_df['labels'] == label]['values']\n",
    "    if len(subset) > 0:  # Only plot if there's data\n",
    "        ax2.hist(subset, bins=bins, alpha=0.6, label=label, \n",
    "                 color=colors[i % len(colors)], density=True)\n",
    "\n",
    "ax2.set_title('Histogram Distribution of Model Scores by Label', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Model Score Values', fontsize=12)\n",
    "ax2.set_ylabel('Density', fontsize=12)\n",
    "ax2.legend(title='True Labels', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary Statistics by Label:\")\n",
    "print(\"=\" * 50)\n",
    "summary_stats = plot_df.groupby('labels')['values'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "print(summary_stats.round(4))\n",
    "\n",
    "# Print value ranges for each label\n",
    "print(\"\\nValue Distribution Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for label in sorted(plot_df['labels'].unique()):\n",
    "    subset = plot_df[plot_df['labels'] == label]['values']\n",
    "    print(f\"{label}: Mean={subset.mean():.3f}, Std={subset.std():.3f}, \"\n",
    "          f\"Range=[{subset.min():.3f}, {subset.max():.3f}], Count={len(subset)}\")\n",
    "\n",
    "# Print threshold analysis for model predictions\n",
    "print(\"\\nThreshold Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Current thresholds used in model:\")\n",
    "print(\"- Entailment: > 0.7\")\n",
    "print(\"- Contradiction: < 0.2\") \n",
    "print(\"- Unverifiable: 0.2 <= score <= 0.7\")\n",
    "print()\n",
    "\n",
    "for threshold in [0.2, 0.5, 0.7]:\n",
    "    above_count = len(plot_df[plot_df['values'] > threshold])\n",
    "    below_count = len(plot_df[plot_df['values'] <= threshold])\n",
    "    print(f\"Score > {threshold}: {above_count} samples ({above_count/len(plot_df)*100:.1f}%)\")\n",
    "    print(f\"Score <= {threshold}: {below_count} samples ({below_count/len(plot_df)*100:.1f}%)\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_loader[\"subtask1_batch2\"].claim.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_reference, edges_reference = read_knowledge_graph_from_pickle(files[0])\n",
    "nodes_claim, edges_claim = read_knowledge_graph_from_pickle(files[1])\n",
    "for key in nodes_claim.keys():\n",
    "    if key in nodes_reference.keys():\n",
    "        print(f\"`{key}`\")\n",
    "    else:\n",
    "        print(f\"Key `{key}` not found in nodes_reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_src, key_tgt in edges_claim.keys():\n",
    "    print(f\"`{key_src}` -> `{key_tgt}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geminillm import gemini_complete_if_cache\n",
    "from json_kv_iml import JsonKVStorage\n",
    "from operationCheatSheet import preprocessing_claim\n",
    "from openaillm import openai_embed\n",
    "\n",
    "LLM_MODEL_NAME = \"gemini-2.5-flash-preview-05-20\"\n",
    "\n",
    "\n",
    "kv_global_config = {\n",
    "    \"working_dir\": \"/tmp\",\n",
    "    \"llm_model_name\": LLM_MODEL_NAME,\n",
    "    \"embedding_batch_num\": 64,  # or another integer suitable for your setup\n",
    "    \"vector_db_storage_cls_kwargs\": {\n",
    "        \"cosine_better_than_threshold\": 0.2  # or another float threshold you want\n",
    "    },\n",
    "    \"base_url\": \"https://api.openai.com/v1\",\n",
    "}\n",
    "\n",
    "llm_cache = JsonKVStorage(namespace=\"llm_cache\", global_config=kv_global_config, embedding_func=openai_embed)\n",
    "    \n",
    "async def llm_wrapper(prompt, history_messages=None, max_tokens=None, **kwargs):\n",
    "    if history_messages is None:\n",
    "        history_messages = []\n",
    "\n",
    "    # Use Google GenAI\n",
    "    return await gemini_complete_if_cache(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        prompt=prompt,\n",
    "        history_messages=history_messages,\n",
    "        hashing_kv=llm_cache,\n",
    "        temperature=0.2,\n",
    "        max_tokens=max_tokens or 1024,\n",
    "    )\n",
    "# answer = dataset_loader[\"subtask1_batch2\"].answer.tolist()[0]\n",
    "# claim = dataset_loader[\"subtask1_batch2\"].claim.tolist()[0]\n",
    "# response = await preprocessing_claim(claim, answer, llm_wrapper, llm_cache, 1024, [])\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answer_batch_2 = dataset_loader[\"subtask1_batch2\"].answer.tolist()\n",
    "all_claim_batch_2 = dataset_loader[\"subtask1_batch2\"].claim.tolist()\n",
    "\n",
    "preprocess_path = \"dataset/subtask1_train_batch2_preprocess.csv\"\n",
    "df = pd.read_csv(preprocess_path)\n",
    "for index, (answer, claim) in enumerate(zip(all_answer_batch_2, all_claim_batch_2)):\n",
    "    if index % 200 == 0:\n",
    "        print(f\"Processing index {index}\")\n",
    "    try:\n",
    "        response = await preprocessing_claim(claim, answer, llm_wrapper, llm_cache, 1024, [])\n",
    "        if response is None:\n",
    "            print(f\"Index {index} has no response\")\n",
    "            continue\n",
    "        else:\n",
    "            df.at[index, \"claim\"] = response\n",
    "    except Exception as e:\n",
    "        print(f\"Index {index} has error\")\n",
    "        print(e)\n",
    "\n",
    "df.to_csv(preprocess_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretty_prompt_compare import PrettyCompare\n",
    "\n",
    "pretty_compare = PrettyCompare(compare_response=True, target=[\"they\", \"They\", \n",
    "                                                              \"them\", \"Them\",\n",
    "                                                              \"their\", \"Their\",\n",
    "                                                              \"theirs\", \"Theirs\",\n",
    "                                                              \"them\", \"Them\",\n",
    "                                                              \"these\", \"These\",\n",
    "                                                              \"those\", \"Those\",\n",
    "                                                              \"this\", \"This\",\n",
    "                                                              \"that\", \"That\",\n",
    "                                                              \"these\", \"These\",\n",
    "                                                              \"those\", \"Those\",\n",
    "                                                              \"this\", \"This\"])\n",
    "\n",
    "claim_processed = df.claim.tolist()\n",
    "all_claim_batch_2 = dataset_loader[\"subtask1_batch2\"].claim.tolist()\n",
    "\n",
    "index = 21\n",
    "\n",
    "all_claim_batch_2[index] |pretty_compare| claim_processed[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[0, \"claim\"] = \"2\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_and_rename_newest_files_by_number():\n",
    "    \"\"\"\n",
    "    Find the newest claim_batch2 files for each number (0-999) and rename them to claim_test format\n",
    "    \"\"\"\n",
    "    outputs_dir = \"outputs\"\n",
    "    \n",
    "    # Pattern to match: claim_batch2_{number}_result_{timestamp}.pkl\n",
    "    pattern = r'claim_batch2_(\\d+)_result_(\\d{8}_\\d{6})\\.(pkl|txt)'\n",
    "    \n",
    "    # Dictionary to store files by number, then by timestamp\n",
    "    files_by_number = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Scan all files in outputs directory\n",
    "    for filename in os.listdir(outputs_dir):\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            timestamp = match.group(2)\n",
    "            extension = match.group(3)\n",
    "            \n",
    "            files_by_number[number][timestamp].append({\n",
    "                'filename': filename,\n",
    "                'number': number,\n",
    "                'timestamp': timestamp,\n",
    "                'extension': extension,\n",
    "                'full_path': os.path.join(outputs_dir, filename)\n",
    "            })\n",
    "    \n",
    "    if not files_by_number:\n",
    "        print(\"No files matching the pattern found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found files for {len(files_by_number)} different numbers\")\n",
    "    \n",
    "    # For each number, find the newest timestamp and rename those files\n",
    "    renamed_count = 0\n",
    "    numbers_processed = []\n",
    "    \n",
    "    for number in sorted(files_by_number.keys(), key=int):\n",
    "        timestamps_for_number = files_by_number[number]\n",
    "        \n",
    "        # Find the newest timestamp for this number\n",
    "        newest_timestamp = max(timestamps_for_number.keys())\n",
    "        newest_files = timestamps_for_number[newest_timestamp]\n",
    "        \n",
    "        print(f\"\\nNumber {number}: Found {len(newest_files)} files with newest timestamp {newest_timestamp}\")\n",
    "        \n",
    "        # Rename the files for this number\n",
    "        for file_info in newest_files:\n",
    "            old_path = file_info['full_path']\n",
    "            \n",
    "            # Create new filename: claim_test_result_{number}_{timestamp}.{extension}\n",
    "            new_filename = f\"claim_test_result_{file_info['number']}_{file_info['timestamp']}.{file_info['extension']}\"\n",
    "            new_path = os.path.join(outputs_dir, new_filename)\n",
    "            \n",
    "            try:\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"  Renamed: {file_info['filename']} -> {new_filename}\")\n",
    "                renamed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error renaming {file_info['filename']}: {e}\")\n",
    "        \n",
    "        numbers_processed.append(number)\n",
    "    \n",
    "    print(f\"\\nSuccessfully renamed {renamed_count} files for {len(numbers_processed)} different numbers!\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\nProcessed numbers: {', '.join(sorted(numbers_processed, key=int))}\")\n",
    "    \n",
    "    # Show which numbers from 0-999 are missing\n",
    "    all_numbers = set(str(i) for i in range(1000))\n",
    "    found_numbers = set(files_by_number.keys())\n",
    "    missing_numbers = all_numbers - found_numbers\n",
    "    \n",
    "    if missing_numbers:\n",
    "        missing_sorted = sorted(missing_numbers, key=int)\n",
    "        print(f\"\\nNumbers with no files found: {', '.join(missing_sorted[:20])}\")\n",
    "        if len(missing_numbers) > 20:\n",
    "            print(f\"... and {len(missing_numbers) - 20} more\")\n",
    "\n",
    "\n",
    "find_and_rename_newest_files_by_number()\n",
    "\n",
    "\n",
    "find_and_rename_newest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_latest_result_by_prefix\n",
    "index  = 0\n",
    "claim_nodes, claim_edges = get_latest_result_by_prefix(f\"claim_test_result_{index}_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def rename_claim_test_files():\n",
    "    \"\"\"\n",
    "    Rename files from claim_test_result_{number}_{timestamp}.{ext} \n",
    "    to claim_test_{number}_result_{timestamp}.{ext}\n",
    "    \"\"\"\n",
    "    outputs_dir = \"outputs\"\n",
    "    \n",
    "    # Pattern to match: claim_test_result_{number}_{timestamp}.{extension}\n",
    "    pattern = r'claim_test_result_(\\d+)_(\\d{8}_\\d{6})\\.(pkl|txt)'\n",
    "    \n",
    "    # Get all files in outputs directory\n",
    "    files_to_rename = []\n",
    "    \n",
    "    for filename in os.listdir(outputs_dir):\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            timestamp = match.group(2)\n",
    "            extension = match.group(3)\n",
    "            \n",
    "            # Create new filename: claim_test_{number}_result_{timestamp}.{extension}\n",
    "            new_filename = f\"claim_test_{number}_result_{timestamp}.{extension}\"\n",
    "            \n",
    "            files_to_rename.append({\n",
    "                'old_filename': filename,\n",
    "                'new_filename': new_filename,\n",
    "                'old_path': os.path.join(outputs_dir, filename),\n",
    "                'new_path': os.path.join(outputs_dir, new_filename)\n",
    "            })\n",
    "    \n",
    "    if not files_to_rename:\n",
    "        print(\"No files matching the pattern 'claim_test_result_*' found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(files_to_rename)} files to rename:\")\n",
    "    for file_info in files_to_rename:\n",
    "        print(f\"  {file_info['old_filename']} -> {file_info['new_filename']}\")\n",
    "    \n",
    "    print(f\"\\nProceeding with renaming {len(files_to_rename)} files...\")\n",
    "    \n",
    "    # Perform the renaming\n",
    "    renamed_count = 0\n",
    "    for file_info in files_to_rename:\n",
    "        try:\n",
    "            os.rename(file_info['old_path'], file_info['new_path'])\n",
    "            print(f\"✓ Renamed: {file_info['old_filename']} -> {file_info['new_filename']}\")\n",
    "            renamed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error renaming {file_info['old_filename']}: {e}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully renamed {renamed_count} out of {len(files_to_rename)} files!\")\n",
    "\n",
    "rename_claim_test_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_latest_result_by_prefix\n",
    "\n",
    "index = 1\n",
    "claim_nodes, claim_edges = get_latest_result_by_prefix(f\"claim_test_{index}_\")\n",
    "\n",
    "claim_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "\"\"\"\n",
    "\n",
    "    **Example 5 (Tempting but actually Entailment):**\n",
    "    Claim: The new AI model helps doctors diagnose diseases more quickly.\n",
    "    Evidence: The recently developed artificial intelligence system was shown in trials to reduce the time required for medical professionals to identify patient conditions.\n",
    "    Analysis:\n",
    "    - Claim's Core Assertion: An AI model speeds up disease diagnosis for doctors.\n",
    "    - Evidence's Key Point: An AI system reduces the time for medical professionals to identify conditions.\n",
    "    - Comparison: \"Doctors\" are \"medical professionals\", \"diagnose diseases\" is \"identify patient conditions\", and \"more quickly\" is \"reduce the time\". The evidence directly supports the gist of the claim, even though the words are different. It is not Unverifiable just because the exact phrasing isn't used.\n",
    "    Classification: Entailment\n",
    "\n",
    "    **Example 6 (Truly Unverifiable):**\n",
    "    Claim: The solar-powered drone broke the world record for flight duration in 2023.\n",
    "    Evidence: A new solar-powered drone has a remarkable flight time, staying airborne for over 96 hours in a recent test.\n",
    "    Analysis:\n",
    "    - Claim's Core Assertion: The drone's flight broke a world record, and this happened in 2023.\n",
    "    - Evidence's Key Point: The drone flew for over 96 hours.\n",
    "    - Comparison: The evidence supports the long flight time, but the **essential components** of \"world record\" and the year \"2023\" are completely missing. Without this information, we cannot verify the claim.\n",
    "    Classification: Unverifiable\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from kg_compare import get_src_tgt_dict, get_map_src_tgt_dict, find_best_span\n",
    "\n",
    "from kg_compare import compare_knowledge_graphs_similarity, find_best_span_for_all_evidences\n",
    "from utils import get_latest_result_by_prefix\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "all_responses = []\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from geminillm import gemini_complete_if_cache\n",
    "from json_kv_iml import JsonKVStorage\n",
    "from operationCheatSheet import preprocessing_claim\n",
    "from openaillm import openai_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geminillm import gemini_complete_if_cache\n",
    "from json_kv_iml import JsonKVStorage\n",
    "from operationCheatSheet import preprocessing_claim\n",
    "from openaillm import openai_embed\n",
    "\n",
    "LLM_MODEL_NAME = \"gemini-2.5-flash-preview-05-20\"\n",
    "\n",
    "kv_global_config = {\n",
    "    \"working_dir\": \"/tmp\",\n",
    "    \"llm_model_name\": LLM_MODEL_NAME,\n",
    "    \"embedding_batch_num\": 64,  # or another integer suitable for your setup\n",
    "    \"vector_db_storage_cls_kwargs\": {\n",
    "        \"cosine_better_than_threshold\": 0.2  # or another float threshold you want\n",
    "    },\n",
    "    \"base_url\": \"https://api.openai.com/v1\",\n",
    "}\n",
    "\n",
    "llm_cache = JsonKVStorage(namespace=\"llm_cache\", global_config=kv_global_config, embedding_func=openai_embed)\n",
    "    \n",
    "async def llm_wrapper(prompt, history_messages=None, max_tokens=None, **kwargs):\n",
    "    if history_messages is None:\n",
    "        history_messages = []\n",
    "\n",
    "    # Use Google GenAI\n",
    "    return await gemini_complete_if_cache(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        prompt=prompt,\n",
    "        history_messages=history_messages,\n",
    "        hashing_kv=llm_cache,\n",
    "        temperature=0.2,\n",
    "        max_tokens=max_tokens or 1024,\n",
    "    )\n",
    "# answer = dataset_loader[\"subtask1_batch2\"].answer.tolist()[0]\n",
    "# claim = dataset_loader[\"subtask1_batch2\"].claim.tolist()[0]\n",
    "# response = await preprocessing_claim(claim, answer, llm_wrapper, llm_cache, 1024, [])\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL_NAME = \"gemini-2.5-flash-preview-05-20\"\n",
    "def write_prediction_to_file(index, row_id, prediction, output_file=\"outputs/response_test.txt\"):\n",
    "    \"\"\"\n",
    "    Write a line with row ID and prediction to the output file\n",
    "    \n",
    "    Args:\n",
    "        row_id: The ID/index of the row being processed\n",
    "        prediction: The prediction result\n",
    "        output_file: Path to the output file\n",
    "    \"\"\"\n",
    "    # Ensure outputs directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Write the prediction with row ID to file\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"{index}, {row_id}, {prediction}\\n\")\n",
    "\n",
    "async def llm_wrapper(prompt, history_messages=None, max_tokens=None, **kwargs):\n",
    "    if history_messages is None:\n",
    "        history_messages = []\n",
    "\n",
    "\n",
    "    # Use Google GenAI\n",
    "    return await gemini_complete_if_cache(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        prompt=prompt,\n",
    "        history_messages=history_messages,\n",
    "        hashing_kv=llm_cache,\n",
    "        temperature=0.2,\n",
    "        max_tokens=max_tokens or 1024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subtask1 for test\n",
      "Processing subtask1 for test_preprocess\n",
      "Processing subtask1 for train_batch2\n",
      "Processing subtask1 for train_batch2_preprocess\n",
      "Processing subtask2 for test\n",
      "Processing subtask2 for test_preprocess\n",
      "Processing subtask2 for train_batch2\n",
      "Processing subtask2 for train_batch2_preprocess\n"
     ]
    }
   ],
   "source": [
    "TASKNAMES = [\"subtask1\", \"subtask2\"]\n",
    "TYPES = [\"test\", \"test_preprocess\", \"train_batch2\", \"train_batch2_preprocess\"]\n",
    "\n",
    "dataset_loader = dict()\n",
    "\n",
    "for taskname in TASKNAMES:\n",
    "    for type in TYPES:\n",
    "        print(f\"Processing {taskname} for {type}\")\n",
    "        prefix = \"dataset\"\n",
    "        # Load the data\n",
    "        \n",
    "        if taskname == \"subtask2\" and type == \"test_preprocess\":\n",
    "            continue\n",
    "        if taskname == \"subtask2\" and type == \"train_batch2_preprocess\":\n",
    "            continue\n",
    "        data = pd.read_csv(f\"{prefix}/{taskname}_{type}.csv\")\n",
    "        dataset_loader[f\"{taskname}_{type}\"] = data\n",
    "\n",
    "dataset_loader[\"subtask1_test\"][\"claim\"] = dataset_loader[\"subtask1_test_preprocess\"][\"claim\"]\n",
    "dataset_loader[\"subtask1_train_batch2\"][\"claim\"] = dataset_loader[\"subtask1_train_batch2_preprocess\"][\"claim\"]\n",
    "\n",
    "output_file = \"outputs/response_test.txt\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 0\n",
      "Processing index 50\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline, AutoModelForSequenceClassification\n",
    "\n",
    "#test_data = dataset_loader[\"subtask1_test\"]\n",
    "\n",
    "# pickle_prefix = \"test\"\n",
    "test_data = dataset_loader[\"subtask1_train_batch2\"][:100]\n",
    "pickle_prefix = \"batch2\"\n",
    "\n",
    "pairs = []\n",
    "value_list = []\n",
    "\n",
    "# Step 1: Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'vectara/hallucination_evaluation_model', trust_remote_code=True)\n",
    "\n",
    "all_responses = []\n",
    "\n",
    "claim_list = []\n",
    "evidence_list = []\n",
    "\n",
    "for index in range(len(test_data)):\n",
    "\n",
    "    if index % 50 == 0:\n",
    "        print(f\"Processing index {index}\")\n",
    "    \n",
    "    row_id = test_data.at[index, \"ID\"]\n",
    "    claim_content = test_data.at[index, \"claim\"]\n",
    "\n",
    "    claim_nodes, claim_edges = await get_latest_result_by_prefix(f\"claim_{pickle_prefix}_{index}_\")\n",
    "    reference_nodes, reference_edges = await get_latest_result_by_prefix(f\"reference_{pickle_prefix}_{index}_\")\n",
    "    reference_content = test_data.at[index, \"reference\"]\n",
    "\n",
    "    claim_node_list = list(claim_nodes.keys())\n",
    "    reference_node_list = list(reference_nodes.keys())\n",
    "\n",
    "    if claim_node_list == [] or reference_node_list == []:\n",
    "        all_responses.append(\"Unverifiable\")\n",
    "        write_prediction_to_file(index, row_id, \"Unverifiable\")\n",
    "        continue\n",
    "        \n",
    "    similarity_matrix = compare_knowledge_graphs_similarity(claim_node_list,\n",
    "                                                            reference_node_list,\n",
    "                                                            if_plot=False)\n",
    "    src_tgt_dict = get_src_tgt_dict(similarity_matrix, claim_node_list, reference_node_list)\n",
    "\n",
    "    claim_reference_edge_map = get_map_src_tgt_dict(src_tgt_dict, claim_edges)\n",
    "\n",
    "    evidence_text = find_best_span_for_all_evidences(reference_edges,\n",
    "                                 claim_reference_edge_map,\n",
    "                                 reference_content)\n",
    "    \n",
    "    if evidence_text == \"\":\n",
    "        evidence_text = reference_content\n",
    "    \n",
    "    claim_list.append(claim_content)\n",
    "    evidence_list.append(evidence_text)\n",
    "\n",
    "    classification_prompt = f\"\"\"\n",
    "    ### INSTRUCTION ###\n",
    "    You are an expert fact-checking AI. Your task is to meticulously analyze a claim against a piece of evidence and classify their relationship. Follow these steps:\n",
    "\n",
    "    1.  **Analyze the Claim**: Identify the core assertion being made in the claim.\n",
    "    2.  **Analyze the Evidence**: Understand what information the evidence provides and what it does not.\n",
    "    3.  **Compare**: Compare the claim's core assertion to the information in the evidence.\n",
    "    4.  **Classify**: Based on your comparison, classify the claim into ONE of the three categories defined below.\n",
    "\n",
    "    ### CATEGORY DEFINITIONS ###\n",
    "\n",
    "    1.  **Entailment**: Choose this if the evidence **directly supports, paraphrases, or is a logical consequence of** the claim. The evidence doesn't need to be a word-for-word match. If the core assertion of the claim is validated by the evidence, it is Entailment.\n",
    "\n",
    "    2.  **Contradiction**: Choose this if the evidence **directly contradicts** the claim. This includes claims that **exaggerate or overstate** what the evidence says (e.g., claim says \"will completely solve\" but evidence says \"will help assist\").\n",
    "\n",
    "    3.  **Unverifiable**: Choose this ONLY if a **key piece of the claim's main assertion is not mentioned** in the evidence, making it impossible to confirm or deny. Do NOT use this just because the wording is different. If the evidence supports the *gist* of the claim, it is likely Entailment, not Unverifiable.\n",
    "\n",
    "    ### EXAMPLES ###\n",
    "\n",
    "    **Example 1 (Clear Entailment):**\n",
    "    Claim: Machine learning in VR can create intelligent agents that adapt to user behavior for a more personalized experience.\n",
    "    Evidence: A novel approach for automated navigation and searching is proposed by incorporating machine learning in virtual reality. An intelligent virtual agent learns objects of interest along with the paths followed for navigation.\n",
    "    Classification: Entailment\n",
    "\n",
    "    **Example 2 (Unverifiable - Key Info Missing):**\n",
    "    Claim: AI-driven VR environments are used for safe training in healthcare and engineering.\n",
    "    Evidence: Virtual Reality frameworks are a promising innovation relevant in areas like preparing test systems, therapeutic and human services, training, and the stimulation industry.\n",
    "    *Reasoning*: The evidence mentions VR for training and healthcare, but it crucially omits any mention of \"AI-driven\" or \"engineering\". A key component of the claim is missing.\n",
    "    Classification: Unverifiable\n",
    "\n",
    "    **Example 3 (Contradiction - Overstatement):**\n",
    "    Claim: Chatbots can assist students in choosing a major, which may **completely eliminate** the stress and confusion associated with this decision.\n",
    "    Evidence: The making of this chatbot aims to **assist** prospective students in determining majors according to their personality. The majors' questionnaire is one way to **assist** students in recommending what majors they should have.\n",
    "    *Reasoning*: The claim's use of \"completely eliminate\" is a massive exaggeration of the evidence's \"assist\". This is a contradiction.\n",
    "    Classification: Contradiction\n",
    "\n",
    "    **Example 4 (Entailment - Paraphrasing/Inference):**\n",
    "    Claim: The system makes it easier for users to navigate virtual spaces.\n",
    "    Evidence: A novel approach for automated navigation and searching is proposed by incorporating machine learning in virtual reality.\n",
    "    *Reasoning*: \"Easier to navigate\" is a reasonable summary of \"automated navigation and searching\". This is Entailment.\n",
    "    Classification: Entailment\n",
    "\n",
    "    ### YOUR TASK ###\n",
    "    Now, perform your step-by-step analysis on the following claim and evidence, then provide your final classification.\n",
    "\n",
    "    Claim:\n",
    "    {claim_content}\n",
    "\n",
    "    Evidence:\n",
    "    {evidence_text}\n",
    "\n",
    "    ### OUTPUT REQUIREMENTS ###\n",
    "    - You MUST respond with ONLY one of the three words: \"Entailment\", \"Contradiction\", or \"Unverifiable\".\n",
    "    - Your entire response must be EXACTLY one of these three words.\n",
    "    - Do NOT provide explanations, justifications, or any additional text in your final output.\n",
    "\n",
    "    Classification:\n",
    "    \"\"\"\n",
    "    response = await llm_wrapper(classification_prompt)\n",
    "    all_responses.append(response)\n",
    "\n",
    "    write_prediction_to_file(index, row_id, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = dataset_loader[\"subtask1_train_batch2\"][:100].copy()\n",
    "\n",
    "label_list = results_df[\"label\"].to_list()\n",
    "\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# value_list = [v.item() if hasattr(v, 'item') else v for v in value_list]\n",
    "\n",
    "\n",
    "# # 1. Create a pandas DataFrame\n",
    "# # This assumes 'value_list' is populated from the previous cell.\n",
    "# df_plot = pd.DataFrame({\n",
    "#     'value': value_list,\n",
    "#     'label': label_list\n",
    "# })\n",
    "\n",
    "# # Convert 'value' column to numeric, coercing errors, and drop any rows that failed conversion\n",
    "# df_plot['value'] = pd.to_numeric(df_plot['value'], errors='coerce')\n",
    "# df_plot.dropna(subset=['value'], inplace=True)\n",
    "\n",
    "# # 2. Create the plots\n",
    "# # Set up the figure with two subplots side-by-side\n",
    "# plt.figure(figsize=(14, 6))\n",
    "\n",
    "# # Plot 1: Distribution density of the entire value list\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.kdeplot(df_plot['value'], fill=True, color=\"skyblue\")\n",
    "# plt.title('Overall Distribution of Values', fontsize=14)\n",
    "# plt.xlabel('Value', fontsize=12)\n",
    "# plt.ylabel('Density', fontsize=12)\n",
    "# plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# # Plot 2: Density for each label type in a unique color\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.kdeplot(data=df_plot, x='value', hue='label', fill=True, common_norm=False, palette='viridis', alpha=0.6)\n",
    "# plt.title('Distribution of Values by Label', fontsize=14)\n",
    "# plt.xlabel('Value', fontsize=12)\n",
    "# plt.ylabel('Density', fontsize=12)\n",
    "# plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# # Adjust layout and display the plots\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Optional: Print summary statistics to understand the data better\n",
    "# print(\"\\\\nSummary Statistics:\")\n",
    "# print(df_plot.groupby('label')['value'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original file\n",
    "with open(\"outputs/response_test.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split each line and extract ID and label\n",
    "full_data = [line.strip().split(\", \")[1:] for line in lines]  # skip the first field (index)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(full_data, columns=[\"ID\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].replace({\n",
    "    \"Unverifiable\": \"unver\",\n",
    "    \"Contradiction\": \"contra\",\n",
    "    \"Entailment\": \"entail\"\n",
    "})\n",
    "\n",
    "# Drop exact duplicate rows (same ID and label)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# df.to_csv(\"outputs/kaggle_results/response_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>claim</th>\n",
       "      <th>reference</th>\n",
       "      <th>label</th>\n",
       "      <th>justification</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s_4</td>\n",
       "      <td>a chatbot enhanced with retrieval capabilities...</td>\n",
       "      <td>** Chatbot Enhanced with Retrieval Capabilitie...</td>\n",
       "      <td>Virtual Assistants are limited in supporting c...</td>\n",
       "      <td>[3]: The widespread use of chatbots is a reali...</td>\n",
       "      <td>contra</td>\n",
       "      <td>The claim directly contradicts the reference b...</td>\n",
       "      <td>contra</td>\n",
       "      <td>unver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s_5</td>\n",
       "      <td>a chatbot enhanced with retrieval capabilities...</td>\n",
       "      <td>** Chatbot Enhanced with Retrieval Capabilitie...</td>\n",
       "      <td>Educational Benefits and Applications: Student...</td>\n",
       "      <td>[4]: Education is one of the important factors...</td>\n",
       "      <td>contra</td>\n",
       "      <td>The claim overstates the statement \"Chatbots m...</td>\n",
       "      <td>contra</td>\n",
       "      <td>contra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s_7</td>\n",
       "      <td>a chatbot enhanced with retrieval capabilities...</td>\n",
       "      <td>** Chatbot Enhanced with Retrieval Capabilitie...</td>\n",
       "      <td>Chatbots should offer responsive interactions,...</td>\n",
       "      <td>[6]: Purpose: Chatbots have been widely adopte...</td>\n",
       "      <td>contra</td>\n",
       "      <td>The claim misinterprets the reference by stati...</td>\n",
       "      <td>contra</td>\n",
       "      <td>contra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s_8</td>\n",
       "      <td>a chatbot enhanced with retrieval capabilities...</td>\n",
       "      <td>** Chatbot Enhanced with Retrieval Capabilitie...</td>\n",
       "      <td>Chatbots can use natural language processing t...</td>\n",
       "      <td>[4]: Education is one of the important factors...</td>\n",
       "      <td>unver</td>\n",
       "      <td>The claim states that \"chatbots might not nece...</td>\n",
       "      <td>unver</td>\n",
       "      <td>unver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s_9</td>\n",
       "      <td>a chatbot enhanced with retrieval capabilities...</td>\n",
       "      <td>** Chatbot Enhanced with Retrieval Capabilitie...</td>\n",
       "      <td>Challenges and Considerations: Technological a...</td>\n",
       "      <td>[3]: The widespread use of chatbots is a reali...</td>\n",
       "      <td>contra</td>\n",
       "      <td>The claim contradicts with the reference by st...</td>\n",
       "      <td>contra</td>\n",
       "      <td>contra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>s_185</td>\n",
       "      <td>What specific security issues arise from incor...</td>\n",
       "      <td>To address the query about specific security i...</td>\n",
       "      <td>Unauthorized Access and Data Breaches: IoT-con...</td>\n",
       "      <td>[1]: The Internet of Things (IoT) enables us t...</td>\n",
       "      <td>entail</td>\n",
       "      <td>The claim is explicitly supported by the refer...</td>\n",
       "      <td>entail</td>\n",
       "      <td>entail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>s_187</td>\n",
       "      <td>What specific security issues arise from incor...</td>\n",
       "      <td>To address the query about specific security i...</td>\n",
       "      <td>Vulnerability to Cyber Attacks: IoT devices ar...</td>\n",
       "      <td>[4]: Internet of Things (IoT) devices present ...</td>\n",
       "      <td>entail</td>\n",
       "      <td>A reference [4] supports the claim completely.</td>\n",
       "      <td>entail</td>\n",
       "      <td>entail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>s_191</td>\n",
       "      <td>What types of interactions are possible with a...</td>\n",
       "      <td>** Types of Interactions with Autonomous Mobil...</td>\n",
       "      <td>Social Interactions: social robots can identif...</td>\n",
       "      <td>[1]: The increasing use of autonomous mobile r...</td>\n",
       "      <td>entail</td>\n",
       "      <td>The claim is supported by the given reference.</td>\n",
       "      <td>entail</td>\n",
       "      <td>unver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>s_192</td>\n",
       "      <td>What types of interactions are possible with a...</td>\n",
       "      <td>** Types of Interactions with Autonomous Mobil...</td>\n",
       "      <td>Human-Robot Interaction (HRI): Robots are inef...</td>\n",
       "      <td>[2]: As a testbed for real-world experimentati...</td>\n",
       "      <td>contra</td>\n",
       "      <td>The claim misinterprets the reference by menti...</td>\n",
       "      <td>contra</td>\n",
       "      <td>unver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>s_195</td>\n",
       "      <td>What types of interactions are possible with a...</td>\n",
       "      <td>** Types of Interactions with Autonomous Mobil...</td>\n",
       "      <td>Multimodal Interfaces: Robots can use various ...</td>\n",
       "      <td>[3]: Purpose: Over the past several years, a v...</td>\n",
       "      <td>contra</td>\n",
       "      <td>The claim is not supported by any of the refer...</td>\n",
       "      <td>contra</td>\n",
       "      <td>contra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                           question  \\\n",
       "0     s_4  a chatbot enhanced with retrieval capabilities...   \n",
       "1     s_5  a chatbot enhanced with retrieval capabilities...   \n",
       "2     s_7  a chatbot enhanced with retrieval capabilities...   \n",
       "3     s_8  a chatbot enhanced with retrieval capabilities...   \n",
       "4     s_9  a chatbot enhanced with retrieval capabilities...   \n",
       "..    ...                                                ...   \n",
       "95  s_185  What specific security issues arise from incor...   \n",
       "96  s_187  What specific security issues arise from incor...   \n",
       "97  s_191  What types of interactions are possible with a...   \n",
       "98  s_192  What types of interactions are possible with a...   \n",
       "99  s_195  What types of interactions are possible with a...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   ** Chatbot Enhanced with Retrieval Capabilitie...   \n",
       "1   ** Chatbot Enhanced with Retrieval Capabilitie...   \n",
       "2   ** Chatbot Enhanced with Retrieval Capabilitie...   \n",
       "3   ** Chatbot Enhanced with Retrieval Capabilitie...   \n",
       "4   ** Chatbot Enhanced with Retrieval Capabilitie...   \n",
       "..                                                ...   \n",
       "95  To address the query about specific security i...   \n",
       "96  To address the query about specific security i...   \n",
       "97  ** Types of Interactions with Autonomous Mobil...   \n",
       "98  ** Types of Interactions with Autonomous Mobil...   \n",
       "99  ** Types of Interactions with Autonomous Mobil...   \n",
       "\n",
       "                                                claim  \\\n",
       "0   Virtual Assistants are limited in supporting c...   \n",
       "1   Educational Benefits and Applications: Student...   \n",
       "2   Chatbots should offer responsive interactions,...   \n",
       "3   Chatbots can use natural language processing t...   \n",
       "4   Challenges and Considerations: Technological a...   \n",
       "..                                                ...   \n",
       "95  Unauthorized Access and Data Breaches: IoT-con...   \n",
       "96  Vulnerability to Cyber Attacks: IoT devices ar...   \n",
       "97  Social Interactions: social robots can identif...   \n",
       "98  Human-Robot Interaction (HRI): Robots are inef...   \n",
       "99  Multimodal Interfaces: Robots can use various ...   \n",
       "\n",
       "                                            reference   label  \\\n",
       "0   [3]: The widespread use of chatbots is a reali...  contra   \n",
       "1   [4]: Education is one of the important factors...  contra   \n",
       "2   [6]: Purpose: Chatbots have been widely adopte...  contra   \n",
       "3   [4]: Education is one of the important factors...   unver   \n",
       "4   [3]: The widespread use of chatbots is a reali...  contra   \n",
       "..                                                ...     ...   \n",
       "95  [1]: The Internet of Things (IoT) enables us t...  entail   \n",
       "96  [4]: Internet of Things (IoT) devices present ...  entail   \n",
       "97  [1]: The increasing use of autonomous mobile r...  entail   \n",
       "98  [2]: As a testbed for real-world experimentati...  contra   \n",
       "99  [3]: Purpose: Over the past several years, a v...  contra   \n",
       "\n",
       "                                        justification true_label  \\\n",
       "0   The claim directly contradicts the reference b...     contra   \n",
       "1   The claim overstates the statement \"Chatbots m...     contra   \n",
       "2   The claim misinterprets the reference by stati...     contra   \n",
       "3   The claim states that \"chatbots might not nece...      unver   \n",
       "4   The claim contradicts with the reference by st...     contra   \n",
       "..                                                ...        ...   \n",
       "95  The claim is explicitly supported by the refer...     entail   \n",
       "96     A reference [4] supports the claim completely.     entail   \n",
       "97     The claim is supported by the given reference.     entail   \n",
       "98  The claim misinterprets the reference by menti...     contra   \n",
       "99  The claim is not supported by any of the refer...     contra   \n",
       "\n",
       "   predicted_label  \n",
       "0            unver  \n",
       "1           contra  \n",
       "2           contra  \n",
       "3            unver  \n",
       "4           contra  \n",
       "..             ...  \n",
       "95          entail  \n",
       "96          entail  \n",
       "97           unver  \n",
       "98           unver  \n",
       "99          contra  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = dataset_loader[\"subtask1_train_batch2\"][:100].copy()\n",
    "results_df[\"true_label\"] = results_df[\"label\"]\n",
    "results_df[\"predicted_label\"]=  df.label[-100:].values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "Total samples processed: 100\n",
      "Weighted F1 Score: 0.6933\n",
      "Accuracy: 0.6800\n",
      "\n",
      "=== DETAILED CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      contra       0.93      0.71      0.81        35\n",
      "      entail       0.91      0.51      0.66        39\n",
      "       unver       0.45      0.88      0.60        26\n",
      "\n",
      "    accuracy                           0.68       100\n",
      "   macro avg       0.76      0.70      0.69       100\n",
      "weighted avg       0.80      0.68      0.69       100\n",
      "\n",
      "\n",
      "=== RESULTS BY BATCH ===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sam2/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'batch'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Show results by batch\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== RESULTS BY BATCH ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.unique():\n\u001b[32m     26\u001b[39m     batch_results = results_df[results_df[\u001b[33m'\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m'\u001b[39m] == batch]\n\u001b[32m     27\u001b[39m     batch_accuracy = (batch_results[\u001b[33m'\u001b[39m\u001b[33mtrue_label\u001b[39m\u001b[33m'\u001b[39m] == batch_results[\u001b[33m'\u001b[39m\u001b[33mpredicted_label\u001b[39m\u001b[33m'\u001b[39m]).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sam2/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sam2/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'batch'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "true_labels = results_df['true_label'].tolist()\n",
    "predicted_labels = results_df['predicted_label'].tolist()\n",
    "\n",
    "# Calculate weighted F1 score (main evaluation metric)\n",
    "weighted_f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"\\n=== EVALUATION RESULTS ===\")\n",
    "print(f\"Total samples processed: {len(results_df)}\")\n",
    "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (results_df['true_label'] == results_df['predicted_label']).mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(f\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "# Show results by batch\n",
    "print(f\"\\n=== RESULTS BY BATCH ===\")\n",
    "for batch in results_df['batch'].unique():\n",
    "    batch_results = results_df[results_df['batch'] == batch]\n",
    "    batch_accuracy = (batch_results['true_label'] == batch_results['predicted_label']).mean()\n",
    "    batch_f1 = f1_score(batch_results['true_label'], batch_results['predicted_label'], average='weighted')\n",
    "    print(f\"{batch}: Accuracy={batch_accuracy:.4f}, Weighted F1={batch_f1:.4f}\")\n",
    "\n",
    "# Calculate F1 scores for batch2 and batch3 separately\n",
    "print(f\"\\n=== INDIVIDUAL BATCH F1 SCORES ===\")\n",
    "if 'batch2' in results_df['batch'].values:\n",
    "    batch2_results = results_df[results_df['batch'] == 'batch2']\n",
    "    batch2_f1 = f1_score(batch2_results['true_label'], batch2_results['predicted_label'], average='weighted')\n",
    "    print(f\"Batch2 Weighted F1 Score: {batch2_f1:.4f}\")\n",
    "\n",
    "if 'batch3' in results_df['batch'].values:\n",
    "    batch3_results = results_df[results_df['batch'] == 'batch3']\n",
    "    batch3_f1 = f1_score(batch3_results['true_label'], batch3_results['predicted_label'], average='weighted')\n",
    "    print(f\"Batch3 Weighted F1 Score: {batch3_f1:.4f}\")\n",
    "\n",
    "# Display first few results\n",
    "print(f\"\\n=== SAMPLE RESULTS ===\")\n",
    "print(results_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
